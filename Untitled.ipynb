{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TV presenter Sarah Beeny was dragged out of the sea topless after being caught in a deadly rip-tide. Talk about 'Wish you were Here'. The guy who rescued her must've thought 'These mermaids are getting fitter. ' She was on holiday wearing just \"a very small G-string\" and sunbathing topless in Tunisia. She's not a small lass, so other people on the beach must've complained that she was blocking the sun. Either way, she decided to move to a better spot along the beach and started to walk there along the water's edge. But she got caught in the strong current and was carried out to sea. She said: \"I got pulled in by the rip-tide wearing a very small G-string and nothing else,\" she said. \"I panicked and screamed loudly.\" She should count herself lucky. If it was Vanessa Feltz she would've been thrown back in.***\n",
      "First off, thank you all so much for the happy well-wishes on my last post! Things got a bit busycrazy last week and then we went out of town for five days. Now we're back! On Saturday we drove to new Jersey to visit with M's family. We had a low-key weekend, Father's day, etc. Happy belated father's day to the dads who read this! On Monday we drove to Philadelphia for a red Sox/Phillies game. We had very little time before the game; only enough to jump on a $2 tourist trolley and take a spin around for 15 minutes before heading out to the ballpark. The game? Not so good. The rain held off, however, and two of the five seats we had were fantastic (the others were pretty good, too). We ate the worst burgers of our lives, got heckled by Phillies fans (who, as it turns out, are not so nice), then crashed at the hotel (where Owen decided to sleep between us perpendicularly; I got the sharp kicky ankles in the back all night). Tuesday we woke up, checked out, grabbed lunch at Geno's, home of the world's most intolerant cheesesteak (anti-immigrant propoganda lining the windows and honest-to-goodness freedom fries, seriously). Then it was Sayonara, Philly, hello six Flags great Adventure. I was never an amusement park person, but M's family was and had made the park a family destination for years. Owen was thrilled to whiz around in the kiddie rides and M rode two roller coasters. As a pregnant liability, I was only allowed to eat and play skee-Ball. Even the most tepid of kiddie rides - five tethered rowboats that slowly rotated in five inches of water - was deemed too dangerous by park officials, and I was left ashore, lest the excitement trigger premature labor and/or lawsuits. The car ride through the parking lot was more harrowing. The Safari park was more my speed, and although they don't let the monkeys assault your vehicle any more (they're safely tucked behind an electrified fence), they didn't seem to have a problem with letting rhinoceroses roam free in packs. Incidentally? You know you've made a smart move in renting a car as giraffe drool runs freely down the passenger side window. After a full day of amorous emus and $12 hamburgers, we headed back to the in-laws' until Wednesday afternoon, and here we are. And there you have it.***\n",
      "Goodness gracious. The weather may not be hot around here, but a couple of tempers certainly are. And for once it's not mine. Bigglest had a job interview today. About forty-five minutes before we needed to leave, I told him I wanted him to carry a cellphone and would he please go find the one the kids carry? Should have been easy, since he's had it all week. Fast-forward to forty-five minutes later. \"Do you have your cell phone?\" No, he doesn't. Much scrabbling about in the back of the car, where he has left his school bag (another no-no) and still no cell phone. I gave him mine. I did not scold him, because I figured that wasn't fair, right before an interview. So I dropped him off and went home. Bigglest has a katana, an aluminum replica of a Japanese samurai's sword. He uses it for a kata he'll be doing in competition. Guess what was in the back of the car, which we keep unlocked? You do not want to know what those damned things cost. Lose it, no replacement, kid. So I took both school bag and katana inside, where I unsuccessfully ransacked the school bag in search of the cell phone. It wasn't there, so I left the bag sitting next to its (neatly stacked -- I'm not vicious) contents. The katana, however, is mine for a while. I haven't decided how long. I will entertain both pleas for mercy and offers to earn it back, but I haven't heard either, yet. Bigglest's nose is seriously out of joint, because he claims that he took the katana inside the last time he had it out. So he can't possibly be responsible for it being in the wrong place. Sure you're not, especially as you have everyone else carefully trained not to touch it, ever. And Bigglest is *really*, stomp- and slam-worthy, pissed off at me. Shrug* I don't do this job for the popularity points. NotSoLittleAnyMore! Is also displeased with me. We have some utility lines marked in our yard, and Middlest chose, for no particular reason, to kick one of the flags out of its place. Annoying, but not the end of the world. When it was brought to my attention, I told her not to do it, and I told her why. But first, NotSoLittleAnyMore! Took justice into her own hands and whacked her with the flag. Middlest was obviously 1 not hurt, and 2 more interested in trying to get her sister in trouble than anything else. However, hitting is not acceptable, and little hitting tends to eventually involve big hitting. So I spoke, kid gloves on, with NotSoLittleAnyMore! Whoa, Nelly! But Middlest did something wrong!! Obviously justice is on NotSoLittleAnyMore!'s side. Okay, hitting is not acceptable. Okay, Mama thinks I should apologize to my sister. You know what *I* think? I think this SUCKS! There is NO justice in THE WORLD! I know BETTER than TO YELL at MAMA, so I'M going TO GO slam some doors AND SCREAM IN THE privacy OF MY room! My word. I'm sure my mother would be shocked, as I was always a model child and never, ever exhibited this sort of behavior. Obviously my children are growing up to be maladjusted. Mama, however, is feeling very well adjusted today, and has sailed serenely through the whole business, which is probably also annoying the Bandar log. It's always easier to feel wounded and unjustly accused when your parent flips out at you. However, I am not feeling cooperative on that front. They'll just have to live with that.***\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import MySQLdb\n",
    "import nltk\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "tagger = PerceptronTagger()\n",
    "\n",
    "#sql='select * from post join post_clean_content where post_clean_content.post_id = post.post_id limit 0,100' #1431071 rows × 14 columns\n",
    "sql='select post_clean_content.content from post_clean_content limit 2,3'\n",
    "#con = sqlite3.connect(\"blogs\")\n",
    "db = MySQLdb.connect(host=\"localhost\", user=\"query\", passwd=\"1234\", db=\"blogs\")\n",
    "df = pandas.read_sql_query(sql, db, index_col=None, coerce_float=True, params=None, parse_dates=None, chunksize=None)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print(row['content'] + \"***\")\n",
    "    \"\"\"\n",
    "    sent_text = nltk.sent_tokenize(row['content'])\n",
    "    for sentence in sent_text:\n",
    "        tokenized_text = nltk.word_tokenize(sentence)\n",
    "        print(tagger.tag(tokenized_text))\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108519L, '50f6c610d7eed88dd8f0413f.xml', '50f6c18fd7eed88dd8f01731', '7384', '9/23/08', datetime.datetime(2008, 9, 24, 1, 13, 13), 'Talia :)', 'http://queentalia.livejournal.com/740.html', 0.0176145, 24L, 74513L, 108519L, \"My first journal hey guys, today I went to practice as usual in the morning. I thought it was ok except for the whole breathing once the whole way to the other side thing. I didnt like that part too much. After that I went to school and I was dressed u for nerd day. I didnt feel too nerdy though since I just through on whatever I could find haha. But that is our little secret. Annie was really dressed up for it though. She could really pass as a nerd. And I did tell her that so its not like I am saying that behind her back. Sarah wasnt here today I dont know why. I sent her a comment but I havent checked if she replied back yet because I am too lazy. If u are really interested in my life than you will find out tomorrow why she wasnt at school. I hope you dont mind that I am using you in my journal Sarah. I am in the habit of typing u instead of you so that is why I have all my u 's underlined because I am trying to stop. I want my journal's to be neat and nice. Ok well anyway today was fun, sort of. Ok I am lying it was acctually pretty boring. I hate when school is like that. Like today felt like Wednesday but I find out its only Tuesday and than the week feels like it is going by so slow. I have a meet this Saturday but I dont know if I am going because the coach hasnt told me and I dont really want to ask. But I do want to go to the swim meet because what is the point of being on the swim team if I cant go to any meets. Ok well this is the end of my first journal. Bye BYE! Talia\", 1507L)\n"
     ]
    }
   ],
   "source": [
    "import MySQLdb\n",
    "sql='select * from post join post_clean_content where post_clean_content.post_id = post.post_id limit 1'\n",
    "db = MySQLdb.connect(host=\"localhost\", user=\"query\", passwd=\"1234\", db=\"blogs\")\n",
    "cursor = db.cursor()\n",
    "cursor.execute(sql)\n",
    "#db.commit()\n",
    "# get the number of rows in the resultset\n",
    "numrows = int(cursor.rowcount)\n",
    "for x in range(0,numrows):\n",
    "    row = cursor.fetchone()\n",
    "    print row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-cb2d525687dc>, line 76)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-cb2d525687dc>\"\u001b[0;36m, line \u001b[0;32m76\u001b[0m\n\u001b[0;31m    for sentence in nltk.sent_tokenize(doc)\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def feature_vector(eg):\n",
    "    row = eg.shape[0]-1\n",
    "    history = row\n",
    "    assert row - history + 1 >= 0 and row>=1\n",
    "    #for each of 16 transitions\n",
    "    #transitions = ['SS' 'SO' 'SX' 'S–' 'OS' 'OO' 'OX' 'O–' 'XS' 'XO' 'XX' 'X–' '–S' '–O' '–X' '––']\n",
    "    transitions = [0]* (16)\n",
    "    states = {\n",
    "        'S': '00',\n",
    "        'O': '01',\n",
    "        'X': '10',\n",
    "        '-': '11'\n",
    "    }\n",
    "    for i in range(row-history+1, row+1):\n",
    "        print(eg[i])\n",
    "        for j in range(0, eg.shape[1]):\n",
    "            a = int(states[eg[i-1][j]] + states[eg[i][j]], 2)\n",
    "            transitions[a]+=1\n",
    "    transitions = [float(x)/(history*eg.shape[1]) for x in transitions]\n",
    "    return transitions\n",
    "\n",
    "\"\"\"\n",
    "def feature_vectors(eg):\n",
    "    history=3\n",
    "    vec = np.array(size=(eg.shape[0]-history+1, 16))\n",
    "    for i in range(history-1, eg.shape[0]):\n",
    "        vec[i] = np.array(feature_vector(eg, i, history))\n",
    "    return vec\n",
    "\"\"\"\n",
    "def perm():\n",
    "    pass\n",
    "\n",
    "def get_postag(sentences):\n",
    "    sentences2 = []\n",
    "    for sentence in sent_text:\n",
    "        nltk.pos_tag(word_tokenize(sentence))\n",
    "        \n",
    "\n",
    "def get_eg(doc, permute):\n",
    "    sentences = nltk.sent_tokenize(row['content'])  \n",
    "    #get all features for this\n",
    "\n",
    "def generate_samples():\n",
    "    data = pandas.read_json('pandas2.txt')\n",
    "    target = []\n",
    "    data = []\n",
    "    for index, row in data.iterrows():\n",
    "        egp = get_eg(row['content'], False)\n",
    "        egn = get_eg(row['content'], True)\n",
    "        data.append(feature_vector(egp))\n",
    "        target.append(1)\n",
    "        data.append(feature_vector(egn))\n",
    "        target.append(0)\n",
    "    return data, target\n",
    "\n",
    "def svm():\n",
    "    from sklearn import svm \n",
    "    clf = svm.SVC(gamma=0.001, C=100.)\n",
    "    data, target = generate_samples()\n",
    "    clf.fit(data[:-1], target[:-1])\n",
    "    return clf\n",
    "\n",
    "def save():\n",
    "    import pickle\n",
    "    model = svm()\n",
    "    pickle.dump(model, open( \"svm.model\", \"wb\" ))\n",
    "\n",
    "def load():\n",
    "    return pickle.load( open( \"svm.model\", \"rb\" ) )\n",
    "    \n",
    "def predict(sentences, model):\n",
    "    #get topic for sentences\n",
    "    topic_arr = gettopics(sentences)\n",
    "    doc_arr = getdocs(topic_arr)\n",
    "    Max = None\n",
    "    Max_sent = None\n",
    "    #for every sentence in the database\n",
    "    for doc in doc_arr:\n",
    "        for sentence in nltk.sent_tokenize(doc):\n",
    "            eg = get_eg(sentences + sentence)\n",
    "            temp = clf.predict(feature_vector(eg))\n",
    "            if Max is None:\n",
    "                Max = temp\n",
    "                Max_sent = sentence\n",
    "            else if Max < temp:\n",
    "                Max = temp\n",
    "                Max_sent = sentence\n",
    "    return Max_sent\n",
    "\n",
    "def LDA():\n",
    "    #cluster entire docs\n",
    "    pass\n",
    "\n",
    "def doc2vec():\n",
    "    pass\n",
    "\n",
    "def sentence(model):#return next sentence\n",
    "    pass\n",
    "\"\"\"\n",
    "eg = np.array([[\"S\",\"O\",\"S\",\"X\",\"O\",\"-\",\"-\",\"-\",\"-\",\"-\",\"-\",\"-\",\"-\",\"-\",\"-\"],\n",
    "               [\"-\",\"-\",\"O\",\"-\",\"-\",\"X\",\"S\",\"O\",\"-\",\"-\",\"-\",\"-\",\"-\",\"-\",\"-\"],\n",
    "               [\"-\",\"-\",\"S\",\"O\",\"-\",\"-\",\"-\",\"-\",\"S\",\"O\",\"O\",\"-\",\"-\",\"-\",\"-\"],\n",
    "               [\"-\",\"-\",\"S\",\"-\",\"-\",\"-\",\"-\",\"-\",\"-\",\"-\",\"-\",\"S\",\"-\",\"-\",\"-\"],\n",
    "               [\"-\",\"-\",\"-\",\"-\",\"-\",\"-\",\"-\",\"-\",\"-\",\"-\",\"-\",\"-\",\"S\",\"O\",\"-\"],\n",
    "               [\"-\",\"X\",\"S\",\"-\",\"-\",\"-\",\"-\",\"-\",\"-\",\"-\",\"-\",\"-\",\"-\",\"-\",\"O\"]])\n",
    "feature_vector(eg)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read \n",
    "import pandas\n",
    "o = pandas.read_json('pandas.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'PRP$'), ('first', 'JJ'), ('journal', 'NN'), ('hey', 'NN'), ('guys', 'NNS'), (',', ','), ('today', 'NN'), ('I', 'PRP'), ('went', 'VBD'), ('to', 'TO'), ('practice', 'NN'), ('as', 'RB'), ('usual', 'JJ'), ('in', 'IN'), ('the', 'DT'), ('morning', 'NN'), ('.', '.')]\n",
      "My/PRP$ first/JJ journal/NN hey/NN guys/NNS ,/, today/NN I/PRP went/VBD to/TO practice/NN as/RB usual/JJ in/IN the/DT morning/NN ./.\n",
      "['N', '', 'N', 'N', 'N', '', 'N', 'N', 'V', 'N', 'N', '', '', '', '', 'N', '']\n",
      "[('I', 'PRP'), ('thought', 'VBD'), ('it', 'PRP'), ('was', 'VBD'), ('ok', 'JJ'), ('except', 'IN'), ('for', 'IN'), ('the', 'DT'), ('whole', 'JJ'), ('breathing', 'NN'), ('once', 'IN'), ('the', 'DT'), ('whole', 'JJ'), ('way', 'NN'), ('to', 'TO'), ('the', 'DT'), ('other', 'JJ'), ('side', 'JJ'), ('thing', 'NN'), ('.', '.')]\n",
      "I/PRP thought/VBD it/PRP was/VBD ok/JJ except/IN for/IN the/DT whole/JJ breathing/NN once/IN the/DT whole/JJ way/NN to/TO the/DT other/JJ side/JJ thing/NN ./.\n",
      "['N', 'V', 'N', 'V', '', '', '', '', '', 'N', '', '', '', 'N', 'N', '', '', '', 'N', '']\n",
      "[('I', 'PRP'), ('didnt', 'VBP'), ('like', 'IN'), ('that', 'DT'), ('part', 'NN'), ('too', 'RB'), ('much', 'JJ'), ('.', '.')]\n",
      "I/PRP didnt/VBP like/IN that/DT part/NN too/RB much/JJ ./.\n",
      "['N', 'V', '', '', 'N', '', '', '']\n",
      "[('After', 'IN'), ('that', 'DT'), ('I', 'PRP'), ('went', 'VBD'), ('to', 'TO'), ('school', 'NN'), ('and', 'CC'), ('I', 'PRP'), ('was', 'VBD'), ('dressed', 'VBN'), ('u', 'NN'), ('for', 'IN'), ('nerd', 'JJ'), ('day', 'NN'), ('.', '.')]\n",
      "After/IN that/DT I/PRP went/VBD to/TO school/NN and/CC I/PRP was/VBD dressed/VBN u/NN for/IN nerd/JJ day/NN ./.\n",
      "['', '', 'N', 'V', 'N', 'N', '', 'N', 'V', 'V', 'N', '', '', 'N', '']\n",
      "[('I', 'PRP'), ('didnt', 'VBP'), ('feel', 'VB'), ('too', 'RB'), ('nerdy', 'JJ'), ('though', 'IN'), ('since', 'IN'), ('I', 'PRP'), ('just', 'RB'), ('through', 'IN'), ('on', 'IN'), ('whatever', 'WDT'), ('I', 'PRP'), ('could', 'MD'), ('find', 'VB'), ('haha', 'NN'), ('.', '.')]\n",
      "I/PRP didnt/VBP feel/VB too/RB nerdy/JJ though/IN since/IN I/PRP just/RB through/IN on/IN whatever/WDT I/PRP could/MD find/VB haha/NN ./.\n",
      "['N', 'V', '', '', '', '', '', 'N', '', '', '', '', 'N', '', '', 'N', '']\n",
      "[('But', 'CC'), ('that', 'DT'), ('is', 'VBZ'), ('our', 'PRP$'), ('little', 'JJ'), ('secret', 'JJ'), ('.', '.')]\n",
      "But/CC that/DT is/VBZ our/PRP$ little/JJ secret/JJ ./.\n",
      "['', '', 'V', 'N', '', '', '']\n",
      "[('Annie', 'NNP'), ('was', 'VBD'), ('really', 'RB'), ('dressed', 'VBN'), ('up', 'RP'), ('for', 'IN'), ('it', 'PRP'), ('though', 'IN'), ('.', '.')]\n",
      "Annie/NNP was/VBD really/RB dressed/VBN up/RP for/IN it/PRP though/IN ./.\n",
      "['N', 'V', '', 'V', '', '', 'N', '', '']\n",
      "[('She', 'PRP'), ('could', 'MD'), ('really', 'RB'), ('pass', 'VB'), ('as', 'IN'), ('a', 'DT'), ('nerd', 'NN'), ('.', '.')]\n",
      "She/PRP could/MD really/RB pass/VB as/IN a/DT nerd/NN ./.\n",
      "['N', '', '', '', '', '', 'N', '']\n",
      "[('And', 'CC'), ('I', 'PRP'), ('did', 'VBD'), ('tell', 'VB'), ('her', 'PRP$'), ('that', 'WDT'), ('so', 'RB'), ('its', 'PRP$'), ('not', 'RB'), ('like', 'IN'), ('I', 'PRP'), ('am', 'VBP'), ('saying', 'VBG'), ('that', 'IN'), ('behind', 'IN'), ('her', 'PRP$'), ('back', 'NN'), ('.', '.')]\n",
      "And/CC I/PRP did/VBD tell/VB her/PRP$ that/WDT so/RB its/PRP$ not/RB like/IN I/PRP am/VBP saying/VBG that/IN behind/IN her/PRP$ back/NN ./.\n",
      "['', 'N', 'V', '', 'N', '', '', 'N', '', '', 'N', 'V', 'V', '', '', 'N', 'N', '']\n",
      "[('Sarah', 'NNP'), ('wasnt', 'NN'), ('here', 'RB'), ('today', 'NN'), ('I', 'PRP'), ('dont', 'VBP'), ('know', 'JJ'), ('why', 'WRB'), ('.', '.')]\n",
      "Sarah/NNP wasnt/NN here/RB today/NN I/PRP dont/VBP know/JJ why/WRB ./.\n",
      "['N', 'N', '', 'N', 'N', 'V', '', '', '']\n",
      "[('I', 'PRP'), ('sent', 'VBD'), ('her', 'PRP'), ('a', 'DT'), ('comment', 'NN'), ('but', 'CC'), ('I', 'PRP'), ('havent', 'VBP'), ('checked', 'VBN'), ('if', 'IN'), ('she', 'PRP'), ('replied', 'VBD'), ('back', 'RB'), ('yet', 'RB'), ('because', 'IN'), ('I', 'PRP'), ('am', 'VBP'), ('too', 'RB'), ('lazy', 'JJ'), ('.', '.')]\n",
      "I/PRP sent/VBD her/PRP a/DT comment/NN but/CC I/PRP havent/VBP checked/VBN if/IN she/PRP replied/VBD back/RB yet/RB because/IN I/PRP am/VBP too/RB lazy/JJ ./.\n",
      "['N', 'V', 'N', '', 'N', '', 'N', 'V', 'V', '', 'N', 'V', '', '', '', 'N', 'V', '', '', '']\n",
      "[('If', 'IN'), ('u', 'JJ'), ('are', 'VBP'), ('really', 'RB'), ('interested', 'JJ'), ('in', 'IN'), ('my', 'PRP$'), ('life', 'NN'), ('than', 'IN'), ('you', 'PRP'), ('will', 'MD'), ('find', 'VB'), ('out', 'RP'), ('tomorrow', 'NN'), ('why', 'WRB'), ('she', 'PRP'), ('wasnt', 'VBD'), ('at', 'IN'), ('school', 'NN'), ('.', '.')]\n",
      "If/IN u/JJ are/VBP really/RB interested/JJ in/IN my/PRP$ life/NN than/IN you/PRP will/MD find/VB out/RP tomorrow/NN why/WRB she/PRP wasnt/VBD at/IN school/NN ./.\n",
      "['', '', 'V', '', '', '', 'N', 'N', '', 'N', '', '', '', 'N', '', 'N', 'V', '', 'N', '']\n",
      "[('I', 'PRP'), ('hope', 'VBP'), ('you', 'PRP'), ('dont', 'VBP'), ('mind', 'IN'), ('that', 'DT'), ('I', 'PRP'), ('am', 'VBP'), ('using', 'VBG'), ('you', 'PRP'), ('in', 'IN'), ('my', 'PRP$'), ('journal', 'NN'), ('Sarah', 'NNP'), ('.', '.')]\n",
      "I/PRP hope/VBP you/PRP dont/VBP mind/IN that/DT I/PRP am/VBP using/VBG you/PRP in/IN my/PRP$ journal/NN Sarah/NNP ./.\n",
      "['N', 'V', 'N', 'V', '', '', 'N', 'V', 'V', 'N', '', 'N', 'N', 'N', '']\n",
      "[('I', 'PRP'), ('am', 'VBP'), ('in', 'IN'), ('the', 'DT'), ('habit', 'NN'), ('of', 'IN'), ('typing', 'VBG'), ('u', 'JJ'), ('instead', 'RB'), ('of', 'IN'), ('you', 'PRP'), ('so', 'VBP'), ('that', 'DT'), ('is', 'VBZ'), ('why', 'WRB'), ('I', 'PRP'), ('have', 'VBP'), ('all', 'DT'), ('my', 'PRP$'), ('u', 'NN'), (\"'s\", 'POS'), ('underlined', 'JJ'), ('because', 'IN'), ('I', 'PRP'), ('am', 'VBP'), ('trying', 'VBG'), ('to', 'TO'), ('stop', 'VB'), ('.', '.')]\n",
      "I/PRP am/VBP in/IN the/DT habit/NN of/IN typing/VBG u/JJ instead/RB of/IN you/PRP so/VBP that/DT is/VBZ why/WRB I/PRP have/VBP all/DT my/PRP$ u/NN 's/POS underlined/JJ because/IN I/PRP am/VBP trying/VBG to/TO stop/VB ./.\n",
      "['N', 'V', '', '', 'N', '', 'V', '', '', '', 'N', 'V', '', 'V', '', 'N', 'V', '', 'N', 'N', '', '', '', 'N', 'V', 'V', 'N', '', '']\n",
      "[('I', 'PRP'), ('want', 'VBP'), ('my', 'PRP$'), ('journal', 'NN'), (\"'s\", 'POS'), ('to', 'TO'), ('be', 'VB'), ('neat', 'JJ'), ('and', 'CC'), ('nice', 'JJ'), ('.', '.')]\n",
      "I/PRP want/VBP my/PRP$ journal/NN 's/POS to/TO be/VB neat/JJ and/CC nice/JJ ./.\n",
      "['N', 'V', 'N', 'N', '', 'N', '', '', '', '', '']\n",
      "[('Ok', 'RB'), ('well', 'RB'), ('anyway', 'RB'), ('today', 'NN'), ('was', 'VBD'), ('fun', 'NN'), (',', ','), ('sort', 'NN'), ('of', 'IN'), ('.', '.')]\n",
      "Ok/RB well/RB anyway/RB today/NN was/VBD fun/NN ,/, sort/NN of/IN ./.\n",
      "['', '', '', 'N', 'V', 'N', '', 'N', '', '']\n",
      "[('Ok', 'NN'), ('I', 'PRP'), ('am', 'VBP'), ('lying', 'VBG'), ('it', 'PRP'), ('was', 'VBD'), ('acctually', 'RB'), ('pretty', 'JJ'), ('boring', 'NN'), ('.', '.')]\n",
      "Ok/NN I/PRP am/VBP lying/VBG it/PRP was/VBD acctually/RB pretty/JJ boring/NN ./.\n",
      "['N', 'N', 'V', 'V', 'N', 'V', '', '', 'N', '']\n",
      "[('I', 'PRP'), ('hate', 'VBP'), ('when', 'WRB'), ('school', 'NN'), ('is', 'VBZ'), ('like', 'IN'), ('that', 'DT'), ('.', '.')]\n",
      "I/PRP hate/VBP when/WRB school/NN is/VBZ like/IN that/DT ./.\n",
      "['N', 'V', '', 'N', 'V', '', '', '']\n",
      "[('Like', 'IN'), ('today', 'NN'), ('felt', 'VBP'), ('like', 'IN'), ('Wednesday', 'NNP'), ('but', 'CC'), ('I', 'PRP'), ('find', 'VBP'), ('out', 'RP'), ('its', 'PRP$'), ('only', 'JJ'), ('Tuesday', 'NNP'), ('and', 'CC'), ('than', 'IN'), ('the', 'DT'), ('week', 'NN'), ('feels', 'NNS'), ('like', 'IN'), ('it', 'PRP'), ('is', 'VBZ'), ('going', 'VBG'), ('by', 'IN'), ('so', 'RB'), ('slow', 'JJ'), ('.', '.')]\n",
      "Like/IN today/NN felt/VBP like/IN Wednesday/NNP but/CC I/PRP find/VBP out/RP its/PRP$ only/JJ Tuesday/NNP and/CC than/IN the/DT week/NN feels/NNS like/IN it/PRP is/VBZ going/VBG by/IN so/RB slow/JJ ./.\n",
      "['', 'N', 'V', '', 'N', '', 'N', 'V', '', 'N', '', 'N', '', '', '', 'N', 'N', '', 'N', 'V', 'V', '', '', '', '']\n",
      "[('I', 'PRP'), ('have', 'VBP'), ('a', 'DT'), ('meet', 'NN'), ('this', 'DT'), ('Saturday', 'NNP'), ('but', 'CC'), ('I', 'PRP'), ('dont', 'VBP'), ('know', 'VBP'), ('if', 'IN'), ('I', 'PRP'), ('am', 'VBP'), ('going', 'VBG'), ('because', 'IN'), ('the', 'DT'), ('coach', 'NN'), ('hasnt', 'NN'), ('told', 'VBD'), ('me', 'PRP'), ('and', 'CC'), ('I', 'PRP'), ('dont', 'VBP'), ('really', 'RB'), ('want', 'VBP'), ('to', 'TO'), ('ask', 'VB'), ('.', '.')]\n",
      "I/PRP have/VBP a/DT meet/NN this/DT Saturday/NNP but/CC I/PRP dont/VBP know/VBP if/IN I/PRP am/VBP going/VBG because/IN the/DT coach/NN hasnt/NN told/VBD me/PRP and/CC I/PRP dont/VBP really/RB want/VBP to/TO ask/VB ./.\n",
      "['N', 'V', '', 'N', '', 'N', '', 'N', 'V', 'V', '', 'N', 'V', 'V', '', '', 'N', 'N', 'V', 'N', '', 'N', 'V', '', 'V', 'N', '', '']\n",
      "[('But', 'CC'), ('I', 'PRP'), ('do', 'VBP'), ('want', 'VB'), ('to', 'TO'), ('go', 'VB'), ('to', 'TO'), ('the', 'DT'), ('swim', 'NN'), ('meet', 'NN'), ('because', 'IN'), ('what', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('point', 'NN'), ('of', 'IN'), ('being', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('swim', 'NN'), ('team', 'NN'), ('if', 'IN'), ('I', 'PRP'), ('cant', 'VBP'), ('go', 'VB'), ('to', 'TO'), ('any', 'DT'), ('meets', 'NNS'), ('.', '.')]\n",
      "But/CC I/PRP do/VBP want/VB to/TO go/VB to/TO the/DT swim/NN meet/NN because/IN what/WP is/VBZ the/DT point/NN of/IN being/VBG on/IN the/DT swim/NN team/NN if/IN I/PRP cant/VBP go/VB to/TO any/DT meets/NNS ./.\n",
      "['', 'N', 'V', '', 'N', '', 'N', '', 'N', 'N', '', '', 'V', '', 'N', '', 'V', '', '', 'N', 'N', '', 'N', 'V', '', 'N', '', 'N', '']\n",
      "[('Ok', 'RB'), ('well', 'RB'), ('this', 'DT'), ('is', 'VBZ'), ('the', 'DT'), ('end', 'NN'), ('of', 'IN'), ('my', 'PRP$'), ('first', 'JJ'), ('journal', 'NN'), ('.', '.')]\n",
      "Ok/RB well/RB this/DT is/VBZ the/DT end/NN of/IN my/PRP$ first/JJ journal/NN ./.\n",
      "['', '', '', 'V', '', 'N', '', 'N', '', 'N', '']\n",
      "[('Bye', 'NNP'), ('BYE', 'NNP'), ('!', '.')]\n",
      "Bye/NNP BYE/NNP !/.\n",
      "['N', 'N', '']\n",
      "[('Talia', 'NN')]\n",
      "Talia/NN\n",
      "['N']\n"
     ]
    }
   ],
   "source": [
    "import nltk, re\n",
    "def getentitygrid(doc):\n",
    "    #get all entities\n",
    "    entities = set()\n",
    "    doc2 = []\n",
    "    for sentence in nltk.sent_tokenize(doc):\n",
    "        sent2 = []\n",
    "        for word, tag in nltk.pos_tag(nltk.word_tokenize(sentence)):\n",
    "            if(tag.startswith(\"N\")):\n",
    "                entities.add(word)\n",
    "            sent2.append((word, tag))\n",
    "        doc2.append(sent2)\n",
    "    \n",
    "    noun_pat = re.compile(r'(?:([a-zA-Z]+)\\/(?:NNPS|NNS|NNP|NN|PRP$|PRP|TO))')\n",
    "    verb_pat = re.compile(r'(?:([a-zA-Z]+)\\/(?:VBD|VBG|VBN|VBP|VBZ)\\s)')\n",
    "    alltoken_pat = re.compile(r'([^\\/\\s]*)\\/([^\\/\\s]*)')\n",
    "    for sentence in doc2:\n",
    "        eg = ['-']*len(entities)\n",
    "        \n",
    "        # do regex on \" \".join[word + \"/\" + tag for word,tag in sentence]\n",
    "        print(sentence)\n",
    "        print(\" \".join([word + \"/\" + tag for (word,tag) in sentence]))\n",
    "        \n",
    "        compund_sent = \" \".join([word + \"/\" + tag for (word,tag) in sentence])\n",
    "        \n",
    "        Map = {}\n",
    "        \n",
    "        count = 0\n",
    "        for m in alltoken_pat.finditer(compund_sent):\n",
    "#             print (m.start(), m.group())\n",
    "            Map[m.start()] = count\n",
    "            count+=1\n",
    "        \n",
    "        grid = ['']*count\n",
    "        \n",
    "        for m in noun_pat.finditer(compund_sent):\n",
    "#             print (m.start(), m.group())\n",
    "            grid[Map[m.start()]] = 'N'\n",
    "            \n",
    "        for m in verb_pat.finditer(compund_sent):\n",
    "#             print (m.start(), m.group())\n",
    "            grid[Map[m.start()]] = 'V'\n",
    "        print(grid)\n",
    "\n",
    "\n",
    "for index, row in o.iterrows():\n",
    "    getentitygrid(row['content'])\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
