{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "from subprocess import call\n",
    "import numpy as np\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.tokenize import StanfordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "FASTTEXT_EXEC_PATH = os.path.abspath(\"./fasttext\")\n",
    "\n",
    "BASE_SNLP_PATH = \"/home/cmps143a1/workspace/sent2vec/stanford/stanford-postagger-2016-10-31\"\n",
    "SNLP_TAGGER_JAR = os.path.join(BASE_SNLP_PATH, \"stanford-postagger.jar\")\n",
    "\n",
    "MODEL_WIKI_UNIGRAMS = os.path.abspath(\"./model.bin\")\n",
    "MODEL_WIKI_BIGRAMS = os.path.abspath(\"./sent2vec_wiki_bigrams\")\n",
    "MODEL_TWITTER_UNIGRAMS = os.path.abspath('./sent2vec_twitter_unigrams')\n",
    "MODEL_TWITTER_BIGRAMS = os.path.abspath('./sent2vec_twitter_bigrams')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(tknzr, sentence, to_lower=True):\n",
    "    \"\"\"Arguments:\n",
    "        - tknzr: a tokenizer implementing the NLTK tokenizer interface\n",
    "        - sentence: a string to be tokenized\n",
    "        - to_lower: lowercasing or not\n",
    "    \"\"\"\n",
    "    sentence = sentence.strip()\n",
    "    sentence = ' '.join([format_token(x) for x in tknzr.tokenize(sentence)])\n",
    "    if to_lower:\n",
    "        sentence = sentence.lower()\n",
    "    sentence = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))','<url>',sentence) #replace urls by <url>\n",
    "    sentence = re.sub('(\\@[^\\s]+)','<user>',sentence) #replace @user268 by <user>\n",
    "    filter(lambda word: ' ' not in word, sentence)\n",
    "    return sentence\n",
    "\n",
    "def format_token(token):\n",
    "    \"\"\"\"\"\"\n",
    "    if token == '-LRB-':\n",
    "        token = '('\n",
    "    elif token == '-RRB-':\n",
    "        token = ')'\n",
    "    elif token == '-RSB-':\n",
    "        token = ']'\n",
    "    elif token == '-LSB-':\n",
    "        token = '['\n",
    "    elif token == '-LCB-':\n",
    "        token = '{'\n",
    "    elif token == '-RCB-':\n",
    "        token = '}'\n",
    "    return token\n",
    "\n",
    "def tokenize_sentences(tknzr, sentences, to_lower=True):\n",
    "    \"\"\"Arguments:\n",
    "        - tknzr: a tokenizer implementing the NLTK tokenizer interface\n",
    "        - sentences: a list of sentences\n",
    "        - to_lower: lowercasing or not\n",
    "    \"\"\"\n",
    "    return [tokenize(tknzr, s, to_lower) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embeddings_for_preprocessed_sentences(sentences, model_path, fasttext_exec_path):\n",
    "    \"\"\"Arguments:\n",
    "        - sentences: a list of preprocessed sentences\n",
    "        - model_path: a path to the sent2vec .bin model\n",
    "        - fasttext_exec_path: a path to the fasttext executable\n",
    "    \"\"\"\n",
    "    timestamp = str(time.time())\n",
    "    test_path = os.path.abspath('./'+timestamp+'_fasttext.test.txt')\n",
    "    embeddings_path = os.path.abspath('./'+timestamp+'_fasttext.embeddings.txt')\n",
    "    dump_text_to_disk(test_path, sentences)\n",
    "    call(fasttext_exec_path+\n",
    "          ' print-vectors '+\n",
    "          model_path + ' < '+\n",
    "          test_path + ' > ' +\n",
    "          embeddings_path, shell=True)\n",
    "    embeddings = read_embeddings(embeddings_path)\n",
    "    os.remove(test_path)\n",
    "    os.remove(embeddings_path)\n",
    "    assert(len(sentences) == len(embeddings))\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def read_embeddings(embeddings_path):\n",
    "    \"\"\"Arguments:\n",
    "        - embeddings_path: path to the embeddings\n",
    "    \"\"\"\n",
    "    with open(embeddings_path, 'r') as in_stream:\n",
    "        embeddings = []\n",
    "        for line in in_stream:\n",
    "            line = '['+line.replace(' ',',')+']'\n",
    "            embeddings.append(eval(line))\n",
    "        return embeddings\n",
    "    return []\n",
    "\n",
    "def dump_text_to_disk(file_path, X, Y=None):\n",
    "    \"\"\"Arguments:\n",
    "        - file_path: where to dump the data\n",
    "        - X: list of sentences to dump\n",
    "        - Y: labels, if any\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as out_stream:\n",
    "        if Y is not None:\n",
    "            for x, y in zip(X, Y):\n",
    "                out_stream.write('__label__'+str(y)+' '+x+' \\n')\n",
    "        else:\n",
    "            for x in X:\n",
    "                out_stream.write(x+' \\n')\n",
    "\n",
    "def get_sentence_embeddings(sentences, ngram='bigrams', model='concat_wiki_twitter'):\n",
    "    \"\"\" Returns a numpy matrix of embeddings for one of the published models. It\n",
    "    handles tokenization and can be given raw sentences.\n",
    "    Arguments:\n",
    "        - ngram: 'unigrams' or 'bigrams'\n",
    "        - model: 'wiki', 'twitter', or 'concat_wiki_twitter'\n",
    "        - sentences: a list of raw sentences ['Once upon a time', 'This is another sentence.', ...]\n",
    "    \"\"\"\n",
    "    wiki_embeddings = None\n",
    "    twitter_embbedings = None\n",
    "    tokenized_sentences_NLTK_tweets = None\n",
    "    tokenized_sentences_SNLP = None\n",
    "    if model == \"wiki\" or model == 'concat_wiki_twitter':\n",
    "#         tknzr = StanfordTokenizer(SNLP_TAGGER_JAR, encoding='utf-8')\n",
    "#         s = ' <delimiter> '.join(sentences) #just a trick to make things faster\n",
    "#         tokenized_sentences_SNLP = tokenize_sentences(tknzr, [s])\n",
    "#         tokenized_sentences_SNLP = tokenized_sentences_SNLP[0].split(' <delimiter> ')\n",
    "        tokenized_sentences_SNLP = sentences\n",
    "        assert(len(tokenized_sentences_SNLP) == len(sentences))\n",
    "        if ngram == 'unigrams':\n",
    "            wiki_embeddings = get_embeddings_for_preprocessed_sentences(tokenized_sentences_SNLP, \\\n",
    "                                     MODEL_WIKI_UNIGRAMS, FASTTEXT_EXEC_PATH)\n",
    "        else:\n",
    "            wiki_embeddings = get_embeddings_for_preprocessed_sentences(tokenized_sentences_SNLP, \\\n",
    "                                     MODEL_WIKI_BIGRAMS, FASTTEXT_EXEC_PATH)\n",
    "    if model == \"twitter\" or model == 'concat_wiki_twitter':\n",
    "        tknzr = TweetTokenizer()\n",
    "        tokenized_sentences_NLTK_tweets = tokenize_sentences(tknzr, sentences)\n",
    "        if ngram == 'unigrams':\n",
    "            twitter_embbedings = get_embeddings_for_preprocessed_sentences(tokenized_sentences_NLTK_tweets, \\\n",
    "                                     MODEL_TWITTER_UNIGRAMS, FASTTEXT_EXEC_PATH)\n",
    "        else:\n",
    "            twitter_embbedings = get_embeddings_for_preprocessed_sentences(tokenized_sentences_NLTK_tweets, \\\n",
    "                                     MODEL_TWITTER_BIGRAMS, FASTTEXT_EXEC_PATH)\n",
    "    if model == \"twitter\":\n",
    "        return twitter_embbedings\n",
    "    elif model == \"wiki\":\n",
    "        return wiki_embeddings\n",
    "    elif model == \"concat_wiki_twitter\":\n",
    "        return np.concatenate((wiki_embeddings, twitter_embbedings), axis=1)\n",
    "    sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0213518142700195\n",
      "(2, 600)\n"
     ]
    }
   ],
   "source": [
    "sentences = ['Once upon a time.', 'And now for something completely different.']\n",
    "\n",
    "import time\n",
    "s = time.time()\n",
    "my_embeddings = get_sentence_embeddings(sentences, ngram='unigrams', model='wiki')\n",
    "print(time.time() - s)\n",
    "print(my_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:1234\n",
      "1:1234\n",
      "2:1234\n",
      "3:1234\n",
      "4:1234\n",
      "5:1234\n",
      "6:1234\n",
      "7:1234\n",
      "8:1234\n",
      "9:1234\n",
      "10:1234\n",
      "11:1234\n",
      "12:1234\n",
      "13:1234\n",
      "14:1234\n",
      "15:1234\n",
      "16:1234\n",
      "17:1234\n",
      "18:1234\n",
      "19:1234\n",
      "20:1234\n",
      "21:1234\n",
      "22:1234\n",
      "23:1234\n",
      "24:1234\n",
      "25:1234\n",
      "26:1234\n",
      "27:1234\n",
      "28:1234\n",
      "29:1234\n",
      "30:1234\n",
      "31:1234\n",
      "32:1234\n",
      "33:1234\n",
      "34:1234\n",
      "35:1234\n",
      "36:1234\n",
      "37:1234\n",
      "38:1234\n",
      "39:1234\n",
      "40:1234\n",
      "41:1234\n",
      "42:1234\n",
      "43:1234\n",
      "44:1234\n",
      "45:1234\n",
      "46:1234\n",
      "47:1234\n",
      "48:1234\n",
      "49:1234\n",
      "50:1234\n",
      "51:1234\n",
      "52:1234\n",
      "53:1234\n",
      "54:1234\n",
      "55:1234\n",
      "56:1234\n",
      "57:1234\n",
      "58:1234\n",
      "59:1234\n",
      "60:1234\n",
      "61:1234\n",
      "62:1234\n",
      "63:1234\n",
      "64:1234\n",
      "65:1234\n",
      "66:1234\n",
      "67:1234\n",
      "68:1234\n",
      "69:1234\n",
      "70:1234\n",
      "71:1234\n",
      "72:1234\n",
      "73:1234\n",
      "74:1234\n",
      "75:1234\n",
      "76:1234\n",
      "77:1234\n",
      "78:1234\n",
      "79:1234\n",
      "80:1234\n",
      "81:1234\n",
      "82:1234\n",
      "83:1234\n",
      "84:1234\n",
      "85:1234\n",
      "86:1234\n",
      "87:1234\n",
      "88:1234\n",
      "89:1234\n",
      "90:1234\n",
      "91:1234\n",
      "92:1234\n",
      "93:1234\n",
      "94:1234\n",
      "95:1234\n",
      "96:1234\n",
      "97:1234\n",
      "98:1234\n",
      "99:1234\n",
      "100:1234\n",
      "101:1234\n",
      "102:1234\n",
      "103:1234\n",
      "104:1234\n",
      "105:1234\n",
      "106:1234\n",
      "107:1234\n",
      "108:1234\n",
      "109:1234\n",
      "110:1234\n",
      "111:1234\n",
      "112:1234\n",
      "113:1234\n",
      "114:1234\n",
      "115:1234\n",
      "116:1234\n",
      "117:1234\n",
      "118:1234\n",
      "119:1234\n",
      "120:1234\n",
      "121:1234\n",
      "122:1234\n",
      "123:1234\n",
      "124:1234\n",
      "125:1234\n",
      "126:1234\n",
      "127:1234\n",
      "128:1234\n",
      "129:1234\n",
      "130:1234\n",
      "131:1234\n",
      "132:1234\n",
      "133:1234\n",
      "134:1234\n",
      "135:1234\n",
      "136:1234\n",
      "137:1234\n",
      "138:1234\n",
      "139:1234\n",
      "140:1234\n",
      "141:1234\n",
      "142:1234\n",
      "143:1234\n",
      "144:1234\n",
      "145:1234\n",
      "146:1234\n",
      "147:1234\n",
      "148:1234\n",
      "149:1234\n",
      "150:1234\n",
      "151:1234\n",
      "152:1234\n",
      "153:1234\n",
      "154:1234\n",
      "155:1234\n",
      "156:1234\n",
      "157:1234\n",
      "158:1234\n",
      "159:1234\n",
      "160:1234\n",
      "161:1234\n",
      "162:1234\n",
      "163:1234\n",
      "164:1234\n",
      "165:1234\n",
      "166:1234\n",
      "167:1234\n",
      "168:1234\n",
      "169:1234\n",
      "170:1234\n",
      "171:1234\n",
      "172:1234\n",
      "173:1234\n",
      "174:1234\n",
      "175:1234\n",
      "176:1234\n",
      "177:1234\n",
      "178:1234\n",
      "179:1234\n",
      "180:1234\n",
      "181:1234\n",
      "182:1234\n",
      "183:1234\n",
      "184:1234\n",
      "185:1234\n",
      "186:1234\n",
      "187:1234\n",
      "188:1234\n",
      "189:1234\n",
      "190:1234\n",
      "191:1234\n",
      "192:1234\n",
      "193:1234\n",
      "194:1234\n",
      "195:1234\n",
      "196:1234\n",
      "197:1234\n",
      "198:1234\n",
      "199:1234\n",
      "200:1234\n",
      "201:1234\n",
      "202:1234\n",
      "203:1234\n",
      "204:1234\n",
      "205:1234\n",
      "206:1234\n",
      "207:1234\n",
      "208:1234\n",
      "209:1234\n",
      "210:1234\n",
      "211:1234\n",
      "212:1234\n",
      "213:1234\n",
      "214:1234\n",
      "215:1234\n",
      "216:1234\n",
      "217:1234\n",
      "218:1234\n",
      "219:1234\n",
      "220:1234\n",
      "221:1234\n",
      "222:1234\n",
      "223:1234\n",
      "224:1234\n",
      "225:1234\n",
      "226:1234\n",
      "227:1234\n",
      "228:1234\n",
      "229:1234\n",
      "230:1234\n",
      "231:1234\n",
      "232:1234\n",
      "233:1234\n",
      "234:1234\n",
      "235:1234\n",
      "236:1234\n",
      "237:1234\n",
      "238:1234\n",
      "239:1234\n",
      "240:1234\n",
      "241:1234\n",
      "242:1234\n",
      "243:1234\n",
      "244:1234\n",
      "245:1234\n",
      "246:1234\n",
      "247:1234\n",
      "248:1234\n",
      "249:1234\n",
      "250:1234\n",
      "251:1234\n",
      "252:1234\n",
      "253:1234\n",
      "254:1234\n",
      "255:1234\n",
      "256:1234\n",
      "257:1234\n",
      "258:1234\n",
      "259:1234\n",
      "260:1234\n",
      "261:1234\n",
      "262:1234\n",
      "263:1234\n",
      "264:1234\n",
      "265:1234\n",
      "266:1234\n",
      "267:1234\n",
      "268:1234\n",
      "269:1234\n",
      "270:1234\n",
      "271:1234\n",
      "272:1234\n",
      "273:1234\n",
      "274:1234\n",
      "275:1234\n",
      "276:1234\n",
      "277:1234\n",
      "278:1234\n",
      "279:1234\n",
      "280:1234\n",
      "281:1234\n",
      "282:1234\n",
      "283:1234\n",
      "284:1234\n",
      "285:1234\n",
      "286:1234\n",
      "287:1234\n",
      "288:1234\n",
      "289:1234\n",
      "290:1234\n",
      "291:1234\n",
      "292:1234\n",
      "293:1234\n",
      "294:1234\n",
      "295:1234\n",
      "296:1234\n",
      "297:1234\n",
      "298:1234\n",
      "299:1234\n",
      "300:1234\n",
      "301:1234\n",
      "302:1234\n",
      "303:1234\n",
      "304:1234\n",
      "305:1234\n",
      "306:1234\n",
      "307:1234\n",
      "308:1234\n",
      "309:1234\n",
      "310:1234\n",
      "311:1234\n",
      "312:1234\n",
      "313:1234\n",
      "314:1234\n",
      "315:1234\n",
      "316:1234\n",
      "317:1234\n",
      "318:1234\n",
      "319:1234\n",
      "320:1234\n",
      "321:1234\n",
      "322:1234\n",
      "323:1234\n",
      "324:1234\n",
      "325:1234\n",
      "326:1234\n",
      "327:1234\n",
      "328:1234\n",
      "329:1234\n",
      "330:1234\n",
      "331:1234\n",
      "332:1234\n",
      "333:1234\n",
      "334:1234\n",
      "335:1234\n",
      "336:1234\n",
      "337:1234\n",
      "338:1234\n",
      "339:1234\n",
      "340:1234\n",
      "341:1234\n",
      "342:1234\n",
      "343:1234\n",
      "344:1234\n",
      "345:1234\n",
      "346:1234\n",
      "347:1234\n",
      "348:1234\n",
      "349:1234\n",
      "350:1234\n",
      "351:1234\n",
      "352:1234\n",
      "353:1234\n",
      "354:1234\n",
      "355:1234\n",
      "356:1234\n",
      "357:1234\n",
      "358:1234\n",
      "359:1234\n",
      "360:1234\n",
      "361:1234\n",
      "362:1234\n",
      "363:1234\n",
      "364:1234\n",
      "365:1234\n",
      "366:1234\n",
      "367:1234\n",
      "368:1234\n",
      "369:1234\n",
      "370:1234\n",
      "371:1234\n",
      "372:1234\n",
      "373:1234\n",
      "374:1234\n",
      "375:1234\n",
      "376:1234\n",
      "377:1234\n",
      "378:1234\n",
      "379:1234\n",
      "380:1234\n",
      "381:1234\n",
      "382:1234\n",
      "383:1234\n",
      "384:1234\n",
      "385:1234\n",
      "386:1234\n",
      "387:1234\n",
      "388:1234\n",
      "389:1234\n",
      "390:1234\n",
      "391:1234\n",
      "392:1234\n",
      "393:1234\n",
      "394:1234\n",
      "395:1234\n",
      "396:1234\n",
      "397:1234\n",
      "398:1234\n",
      "399:1234\n",
      "400:1234\n",
      "401:1234\n",
      "402:1234\n",
      "403:1234\n",
      "404:1234\n",
      "405:1234\n",
      "406:1234\n",
      "407:1234\n",
      "408:1234\n",
      "409:1234\n",
      "410:1234\n",
      "411:1234\n",
      "412:1234\n",
      "413:1234\n",
      "414:1234\n",
      "415:1234\n",
      "416:1234\n",
      "417:1234\n",
      "418:1234\n",
      "419:1234\n",
      "420:1234\n",
      "421:1234\n",
      "422:1234\n",
      "423:1234\n",
      "424:1234\n",
      "425:1234\n",
      "426:1234\n",
      "427:1234\n",
      "428:1234\n",
      "429:1234\n",
      "430:1234\n",
      "431:1234\n",
      "432:1234\n",
      "433:1234\n",
      "434:1234\n",
      "435:1234\n",
      "436:1234\n",
      "437:1234\n",
      "438:1234\n",
      "439:1234\n",
      "440:1234\n",
      "441:1234\n",
      "442:1234\n",
      "443:1234\n",
      "444:1234\n",
      "445:1234\n",
      "446:1234\n",
      "447:1234\n",
      "448:1234\n",
      "449:1234\n",
      "450:1234\n",
      "451:1234\n",
      "452:1234\n",
      "453:1234\n",
      "454:1234\n",
      "455:1234\n",
      "456:1234\n",
      "457:1234\n",
      "458:1234\n",
      "459:1234\n",
      "460:1234\n",
      "461:1234\n",
      "462:1234\n",
      "463:1234\n",
      "464:1234\n",
      "465:1234\n",
      "466:1234\n",
      "467:1234\n",
      "468:1234\n",
      "469:1234\n",
      "470:1234\n",
      "471:1234\n",
      "472:1234\n",
      "473:1234\n",
      "474:1234\n",
      "475:1234\n",
      "476:1234\n",
      "477:1234\n",
      "478:1234\n",
      "479:1234\n",
      "480:1234\n",
      "481:1234\n",
      "482:1234\n",
      "483:1234\n",
      "484:1234\n",
      "485:1234\n",
      "486:1234\n",
      "487:1234\n",
      "488:1234\n",
      "489:1234\n",
      "490:1234\n",
      "491:1234\n",
      "492:1234\n",
      "493:1234\n",
      "494:1234\n",
      "495:1234\n",
      "496:1234\n",
      "497:1234\n",
      "498:1234\n",
      "499:1234\n",
      "500:1234\n",
      "501:1234\n",
      "502:1234\n",
      "503:1234\n",
      "504:1234\n",
      "505:1234\n",
      "506:1234\n",
      "507:1234\n",
      "508:1234\n",
      "509:1234\n",
      "510:1234\n",
      "511:1234\n",
      "512:1234\n",
      "513:1234\n",
      "514:1234\n",
      "515:1234\n",
      "516:1234\n",
      "517:1234\n",
      "518:1234\n",
      "519:1234\n",
      "520:1234\n",
      "521:1234\n",
      "522:1234\n",
      "523:1234\n",
      "524:1234\n",
      "525:1234\n",
      "526:1234\n",
      "527:1234\n",
      "528:1234\n",
      "529:1234\n",
      "530:1234\n",
      "531:1234\n",
      "532:1234\n",
      "533:1234\n",
      "534:1234\n",
      "535:1234\n",
      "536:1234\n",
      "537:1234\n",
      "538:1234\n",
      "539:1234\n",
      "540:1234\n",
      "541:1234\n",
      "542:1234\n",
      "543:1234\n",
      "544:1234\n",
      "545:1234\n",
      "546:1234\n",
      "547:1234\n",
      "548:1234\n",
      "549:1234\n",
      "550:1234\n",
      "551:1234\n",
      "552:1234\n",
      "553:1234\n",
      "554:1234\n",
      "555:1234\n",
      "556:1234\n",
      "557:1234\n",
      "558:1234\n",
      "559:1234\n",
      "560:1234\n",
      "561:1234\n",
      "562:1234\n",
      "563:1234\n",
      "564:1234\n",
      "565:1234\n",
      "566:1234\n",
      "567:1234\n",
      "568:1234\n",
      "569:1234\n",
      "570:1234\n",
      "571:1234\n",
      "572:1234\n",
      "573:1234\n",
      "574:1234\n",
      "575:1234\n",
      "576:1234\n",
      "577:1234\n",
      "578:1234\n",
      "579:1234\n",
      "580:1234\n",
      "581:1234\n",
      "582:1234\n",
      "583:1234\n",
      "584:1234\n",
      "585:1234\n",
      "586:1234\n",
      "587:1234\n",
      "588:1234\n",
      "589:1234\n",
      "590:1234\n",
      "591:1234\n",
      "592:1234\n",
      "593:1234\n",
      "594:1234\n",
      "595:1234\n",
      "596:1234\n",
      "597:1234\n",
      "598:1234\n",
      "599:1234\n",
      "600:1234\n",
      "601:1234\n",
      "602:1234\n",
      "603:1234\n",
      "604:1234\n",
      "605:1234\n",
      "606:1234\n",
      "607:1234\n",
      "608:1234\n",
      "609:1234\n",
      "610:1234\n",
      "611:1234\n",
      "612:1234\n",
      "613:1234\n",
      "614:1234\n",
      "615:1234\n",
      "616:1234\n",
      "617:1234\n",
      "618:1234\n",
      "619:1234\n",
      "620:1234\n",
      "621:1234\n",
      "622:1234\n",
      "623:1234\n",
      "624:1234\n",
      "625:1234\n",
      "626:1234\n",
      "627:1234\n",
      "628:1234\n",
      "629:1234\n",
      "630:1234\n",
      "631:1234\n",
      "632:1234\n",
      "633:1234\n",
      "634:1234\n",
      "635:1234\n",
      "636:1234\n",
      "637:1234\n",
      "638:1234\n",
      "639:1234\n",
      "640:1234\n",
      "641:1234\n",
      "642:1234\n",
      "643:1234\n",
      "644:1234\n",
      "645:1234\n",
      "646:1234\n",
      "647:1234\n",
      "648:1234\n",
      "649:1234\n",
      "650:1234\n",
      "651:1234\n",
      "652:1234\n",
      "653:1234\n",
      "654:1234\n",
      "655:1234\n",
      "656:1234\n",
      "657:1234\n",
      "658:1234\n",
      "659:1234\n",
      "660:1234\n",
      "661:1234\n",
      "662:1234\n",
      "663:1234\n",
      "664:1234\n",
      "665:1234\n",
      "666:1234\n",
      "667:1234\n",
      "668:1234\n",
      "669:1234\n",
      "670:1234\n",
      "671:1234\n",
      "672:1234\n",
      "673:1234\n",
      "674:1234\n",
      "675:1234\n",
      "676:1234\n",
      "677:1234\n",
      "678:1234\n",
      "679:1234\n",
      "680:1234\n",
      "681:1234\n",
      "682:1234\n",
      "683:1234\n",
      "684:1234\n",
      "685:1234\n",
      "686:1234\n",
      "687:1234\n",
      "688:1234\n",
      "689:1234\n",
      "690:1234\n",
      "691:1234\n",
      "692:1234\n",
      "693:1234\n",
      "694:1234\n",
      "695:1234\n",
      "696:1234\n",
      "697:1234\n",
      "698:1234\n",
      "699:1234\n",
      "700:1234\n",
      "701:1234\n",
      "702:1234\n",
      "703:1234\n",
      "704:1234\n",
      "705:1234\n",
      "706:1234\n",
      "707:1234\n",
      "708:1234\n",
      "709:1234\n",
      "710:1234\n",
      "711:1234\n",
      "712:1234\n",
      "713:1234\n",
      "714:1234\n",
      "715:1234\n",
      "716:1234\n",
      "717:1234\n",
      "718:1234\n",
      "719:1234\n",
      "720:1234\n",
      "721:1234\n",
      "722:1234\n",
      "723:1234\n",
      "724:1234\n",
      "725:1234\n",
      "726:1234\n",
      "727:1234\n",
      "728:1234\n",
      "729:1234\n",
      "730:1234\n",
      "731:1234\n",
      "732:1234\n",
      "733:1234\n",
      "734:1234\n",
      "735:1234\n",
      "736:1234\n",
      "737:1234\n",
      "738:1234\n",
      "739:1234\n",
      "740:1234\n",
      "741:1234\n",
      "742:1234\n",
      "743:1234\n",
      "744:1234\n",
      "745:1234\n",
      "746:1234\n",
      "747:1234\n",
      "748:1234\n",
      "749:1234\n",
      "750:1234\n",
      "751:1234\n",
      "752:1234\n",
      "753:1234\n",
      "754:1234\n",
      "755:1234\n",
      "756:1234\n",
      "757:1234\n",
      "758:1234\n",
      "759:1234\n",
      "760:1234\n",
      "761:1234\n",
      "762:1234\n",
      "763:1234\n",
      "764:1234\n",
      "765:1234\n",
      "766:1234\n",
      "767:1234\n",
      "768:1234\n",
      "769:1234\n",
      "770:1234\n",
      "771:1234\n",
      "772:1234\n",
      "773:1234\n",
      "774:1234\n",
      "775:1234\n",
      "776:1234\n",
      "777:1234\n",
      "778:1234\n",
      "779:1234\n",
      "780:1234\n",
      "781:1234\n",
      "782:1234\n",
      "783:1234\n",
      "784:1234\n",
      "785:1234\n",
      "786:1234\n",
      "787:1234\n",
      "788:1234\n",
      "789:1234\n",
      "790:1234\n",
      "791:1234\n",
      "792:1234\n",
      "793:1234\n",
      "794:1234\n",
      "795:1234\n",
      "796:1234\n",
      "797:1234\n",
      "798:1234\n",
      "799:1234\n",
      "800:1234\n",
      "801:1234\n",
      "802:1234\n",
      "803:1234\n",
      "804:1234\n",
      "805:1234\n",
      "806:1234\n",
      "807:1234\n",
      "808:1234\n",
      "809:1234\n",
      "810:1234\n",
      "811:1234\n",
      "812:1234\n",
      "813:1234\n",
      "814:1234\n",
      "815:1234\n",
      "816:1234\n",
      "817:1234\n",
      "818:1234\n",
      "819:1234\n",
      "820:1234\n",
      "821:1234\n",
      "822:1234\n",
      "823:1234\n",
      "824:1234\n",
      "825:1234\n",
      "826:1234\n",
      "827:1234\n",
      "828:1234\n",
      "829:1234\n",
      "830:1234\n",
      "831:1234\n",
      "832:1234\n",
      "833:1234\n",
      "834:1234\n",
      "835:1234\n",
      "836:1234\n",
      "837:1234\n",
      "838:1234\n",
      "839:1234\n",
      "840:1234\n",
      "841:1234\n",
      "842:1234\n",
      "843:1234\n",
      "844:1234\n",
      "845:1234\n",
      "846:1234\n",
      "847:1234\n",
      "848:1234\n",
      "849:1234\n",
      "850:1234\n",
      "851:1234\n",
      "852:1234\n",
      "853:1234\n",
      "854:1234\n",
      "855:1234\n",
      "856:1234\n",
      "857:1234\n",
      "858:1234\n",
      "859:1234\n",
      "860:1234\n",
      "861:1234\n",
      "862:1234\n",
      "863:1234\n",
      "864:1234\n",
      "865:1234\n",
      "866:1234\n",
      "867:1234\n",
      "868:1234\n",
      "869:1234\n",
      "870:1234\n",
      "871:1234\n",
      "872:1234\n",
      "873:1234\n",
      "874:1234\n",
      "875:1234\n",
      "876:1234\n",
      "877:1234\n",
      "878:1234\n",
      "879:1234\n",
      "880:1234\n",
      "881:1234\n",
      "882:1234\n",
      "883:1234\n",
      "884:1234\n",
      "885:1234\n",
      "886:1234\n",
      "887:1234\n",
      "888:1234\n",
      "889:1234\n",
      "890:1234\n",
      "891:1234\n",
      "892:1234\n",
      "893:1234\n",
      "894:1234\n",
      "895:1234\n",
      "896:1234\n",
      "897:1234\n",
      "898:1234\n",
      "899:1234\n",
      "900:1234\n",
      "901:1234\n",
      "902:1234\n",
      "903:1234\n",
      "904:1234\n",
      "905:1234\n",
      "906:1234\n",
      "907:1234\n",
      "908:1234\n",
      "909:1234\n",
      "910:1234\n",
      "911:1234\n",
      "912:1234\n",
      "913:1234\n",
      "914:1234\n",
      "915:1234\n",
      "916:1234\n",
      "917:1234\n",
      "918:1234\n",
      "919:1234\n",
      "920:1234\n",
      "921:1234\n",
      "922:1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "923:1234\n",
      "924:1234\n",
      "925:1234\n",
      "926:1234\n",
      "927:1234\n",
      "928:1234\n",
      "929:1234\n",
      "930:1234\n",
      "931:1234\n",
      "932:1234\n",
      "933:1234\n",
      "934:1234\n",
      "935:1234\n",
      "936:1234\n",
      "937:1234\n",
      "938:1234\n",
      "939:1234\n",
      "940:1234\n",
      "941:1234\n",
      "942:1234\n",
      "943:1234\n",
      "944:1234\n",
      "945:1234\n",
      "946:1234\n",
      "947:1234\n",
      "948:1234\n",
      "949:1234\n",
      "950:1234\n",
      "951:1234\n",
      "952:1234\n",
      "953:1234\n",
      "954:1234\n",
      "955:1234\n",
      "956:1234\n",
      "957:1234\n",
      "958:1234\n",
      "959:1234\n",
      "960:1234\n",
      "961:1234\n",
      "962:1234\n",
      "963:1234\n",
      "964:1234\n",
      "965:1234\n",
      "966:1234\n",
      "967:1234\n",
      "968:1234\n",
      "969:1234\n",
      "970:1234\n",
      "971:1234\n",
      "972:1234\n",
      "973:1234\n",
      "974:1234\n",
      "975:1234\n",
      "976:1234\n",
      "977:1234\n",
      "978:1234\n",
      "979:1234\n",
      "980:1234\n",
      "981:1234\n",
      "982:1234\n",
      "983:1234\n",
      "984:1234\n",
      "985:1234\n",
      "986:1234\n",
      "987:1234\n",
      "988:1234\n",
      "989:1234\n",
      "990:1234\n",
      "991:1234\n",
      "992:1234\n",
      "993:1234\n",
      "994:1234\n",
      "995:1234\n",
      "996:1234\n",
      "997:1234\n",
      "998:1234\n",
      "999:1234\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0c524465cfa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"stormembeddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "def absoluteFilePaths(directory):\n",
    "    for dirpath,_,filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "            yield os.path.abspath(os.path.join(dirpath, f))\n",
    "\n",
    "blog = ['storm']\n",
    "docs = []\n",
    "for f in [f for mypath in blog for f in list(absoluteFilePaths('blogs_topic_sorted/blogs_topic_sorted/'+mypath+'/train'))]:\n",
    "    with open(f, 'r') as ff:\n",
    "        docs.append(ff.read())\n",
    "\n",
    "embeddings = []\n",
    "i=0\n",
    "for doc in docs[0:1000]:\n",
    "    sents = []\n",
    "    for sent in nltk.sent_tokenize(doc):\n",
    "        sents.append(sent)\n",
    "    embeddings.append(get_sentence_embeddings(sents, ngram='unigrams', model='wiki'))\n",
    "    print(str(i) + \":\"+str(len(docs)))\n",
    "    sys.stdout.flush()    \n",
    "    i+=1\n",
    "pickle.dump( embeddings, open( \"stormembeddings\", \"wb\" ) )\n",
    "print(\"done\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "embeddings = pickle.load( open( \"stormembeddings\", \"rb\" ) )\n",
    "# embeddings = embeddings[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def createrand(embeddings):\n",
    "    size = 0\n",
    "    for doc in embeddings:\n",
    "        size += (len(doc)-1)\n",
    "    doc_no = np.random.randint(0, len(embeddings), size=size)\n",
    "    \n",
    "    jagged = []\n",
    "    size=0\n",
    "    for doc in embeddings:\n",
    "        a = []\n",
    "        for sent_idx in range(0, len(doc)-1):\n",
    "            a.append(np.random.randint(0, len(embeddings[doc_no[size]]), size=1)[0])\n",
    "            size+=1\n",
    "        jagged.append(a)\n",
    "    return doc_no, jagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "doc_no, jagged = createrand(embeddings)\n",
    "X = []\n",
    "Y = []\n",
    "size=0\n",
    "for doc_idx in range(0, len(embeddings)):\n",
    "    doc = embeddings[doc_idx]\n",
    "    for sent_idx in range(0, len(doc)-1):\n",
    "        #positive\n",
    "        X.append(np.append(doc[sent_idx], doc[sent_idx+1]))\n",
    "        Y.append(1)\n",
    "        #negative\n",
    "        #for this sent get any arbitrary sent in corpus\n",
    "        a = doc_no[size]\n",
    "        b = jagged[doc_idx][sent_idx]\n",
    "        X.append(np.append(doc[sent_idx], embeddings[a][b]))\n",
    "        Y.append(0)\n",
    "        size+=1\n",
    "\n",
    "import pickle\n",
    "pickle.dump( (X,Y), open( \"svm.data\", \"wb\" ) )\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import pickle\n",
    "X,Y = pickle.load( open( \"svm.data\", \"rb\" ) )\n",
    "clf = svm.SVC()\n",
    "\n",
    "print(\"fitting\")\n",
    "sys.stdout.flush()\n",
    "clf.fit(X, Y)\n",
    "pickle.dump( clf, open( \"svm.model1111\", \"wb\" ) )\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred = 0, tar = 0: 305\n",
      "pred = 0, tar = 1: 186\n",
      "pred = 1, tar = 0: 188\n",
      "pred = 1, tar = 1: 321\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "X,Y = pickle.load( open( \"svm.data\", \"rb\" ) )\n",
    "X = np.array([np.array(xi) for xi in X])\n",
    "Y = np.array([np.array(xi) for xi in Y])\n",
    "clf = pickle.load( open( \"svm.model1111\", \"rb\" ) )\n",
    "\n",
    "def getAccuracy(X, Y, clf): \n",
    "    \n",
    "    rand_index = np.random.choice(len(X), size=1000)\n",
    "    X = X[rand_index]\n",
    "    Y = Y[rand_index]\n",
    "    ypred = clf.predict(X)\n",
    "    count1 = 0 \n",
    "    count2 = 0 \n",
    "    count3 = 0 \n",
    "    count4 = 0 \n",
    "    for i in range(0, 1000):\n",
    "        if(ypred[i] == 0): \n",
    "            if(Y[i] == 0): \n",
    "                count1 += 1\n",
    "            else:\n",
    "                count2 += 1\n",
    "        else:\n",
    "            if(Y[i] == 0): \n",
    "                count3 += 1\n",
    "            else:\n",
    "                count4 += 1\n",
    "    sys.stdout.write(\"pred = 0, tar = 0: \" + str(count1) + \"\\n\")\n",
    "    sys.stdout.write(\"pred = 0, tar = 1: \" + str(count2) + \"\\n\")\n",
    "    sys.stdout.write(\"pred = 1, tar = 0: \" + str(count3) + \"\\n\")\n",
    "    sys.stdout.write(\"pred = 1, tar = 1: \" + str(count4) + \"\\n\")\n",
    "    sys.stdout.write(\"done\\n\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "getAccuracy(X, Y, clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
